{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nifti_file(file_path):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    img = sitk.ReadImage(file_path)\n",
    "    # 轉為 NumPy 陣列\n",
    "    img_arr = sitk.GetArrayFromImage(img)\n",
    "\n",
    "    return img_arr\n",
    "\n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 64\n",
    "    desired_width = 128\n",
    "    desired_height = 128\n",
    "    \n",
    "    # Get current depth\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    current_depth = img.shape[-1]\n",
    "    \n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    img = ndimage.rotate(img, 90, reshape=False)\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img\n",
    "\n",
    "def preprocess_img(file_path):\n",
    "    img = read_nifti_file(file_path)\n",
    "    return resize_volume(img)\n",
    "\n",
    "def show_slices(slices):\n",
    "    fig, axes = plt.subplots(1, len(slices))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice, cmap=\"gray\", origin=\"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:\\\\Users\\\\Gina\\\\Lab\\\\kidney\\\\nnUNet-1\\\\Result'\n",
    "mask_path = os.path.join(data_dir, 'case_00001.nii.gz')\n",
    "\n",
    "data = read_nifti_file(mask_path)\n",
    "resize_data = resize_volume(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image (black): 156347323\n",
      "original image (kidney): 1446043\n",
      "original image (tumor): (array([100, 100, 101, ..., 147, 147, 147], dtype=int64), array([297, 297, 294, ..., 273, 273, 273], dtype=int64), array([339, 340, 338, ..., 263, 264, 265], dtype=int64))\n",
      "--------------------\n",
      "resize image (black): 1039098\n",
      "resize image (kidney): 9369\n",
      "resize image (tumor): 3\n"
     ]
    }
   ],
   "source": [
    "print('original image (black):', len(np.where(data == 0)[0]))\n",
    "print('original image (kidney):', len(np.where(data == 1)[-1]))\n",
    "print('original image (tumor):', np.where(data == 2))\n",
    "print('-' * 20)\n",
    "print('resize image (black):', len(np.where(resize_data == 0)[0]))\n",
    "print('resize image (kidney):', len(np.where(resize_data == 1)[-1]))\n",
    "print('resize image (tumor):', len(np.where(resize_data == 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(img):\n",
    "    tumor_region = np.where(img == 2)\n",
    "\n",
    "    if len(tumor_region[0]) == 0:\n",
    "        return img\n",
    "\n",
    "    l_width = min(tumor_region[0])\n",
    "    l_height = min(tumor_region[1])\n",
    "    l_depth = min(tumor_region[2])\n",
    "\n",
    "    r_width = max(tumor_region[0])\n",
    "    r_height = max(tumor_region[1])\n",
    "    r_depth = max(tumor_region[2])\n",
    "\n",
    "    # turn the kidney part to black\n",
    "    img[np.where(img == 1)] = 0\n",
    "\n",
    "    return img[l_width:r_width, l_height:r_height, l_depth:r_depth]\n",
    "\n",
    "def get_tumor_region(file_path):\n",
    "    img = read_nifti_file(file_path)\n",
    "    return get_bounding_box(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([100, 100, 101, ..., 147, 147, 147], dtype=int64), array([297, 297, 294, ..., 273, 273, 273], dtype=int64), array([339, 340, 338, ..., 263, 264, 265], dtype=int64))\n",
      "image (tumor): (array([ 0,  0,  1, ..., 46, 46, 46], dtype=int64), array([43, 43, 40, ..., 21, 21, 21], dtype=int64), array([85, 86, 84, ..., 10, 11, 12], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAC9CAYAAACZOYZcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAQtElEQVR4nO3dT2hT6dvG8Tu1aVpKerAUG2OrVhBcFB0mKkwR6z/qoiLuisjsK6ZUutJVZTbJWhiRkaGbgenGLlwMMhE1KF04JBRTBVdOW7UhONiTitqouX+LeT2vabQmNTbPyXw/cC968tQ+T8+V4aJpph5VVQEAADBIXbU3AAAAsBwFBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOPXV3sBq5PN5efbsmfj9fvF4PNXeDlxKVWVxcVGCwaDU1a1NVye7qASyC7cqJ7uuLCjPnj2Tzs7Oam8DNWJubk46OjrW5GuRXVQS2YVblZJdV77E4/f7q70F1JC1zBPZRSWRXbhVKXlyZUHhx4uopLXME9lFJZFduFUpeXJlQQEAALWNggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABjnqwpKJBIRj8cjZ8+eda6pqly4cEGCwaA0NTXJgQMH5MGDBwWft7S0JENDQ9LW1ibNzc1y/PhxefLkyddsBfhqZBduRXZRk3SV7t27p1u3btWdO3fq8PCwcz0ajarf79erV69qKpXSgYEB3bhxo2azWWfN4OCgbtq0SWOxmCaTST148KDu2rVL3717V9LXtm1bRYRhKjK2bZNdxpVDdhm3zofsrmRVBWVxcVG3b9+usVhMe3t7nYKSz+c1EAhoNBp11r5580Yty9LLly+rqurCwoJ6vV4dHx931jx9+lTr6ur0+vXrJX19nihMJce2bbLLuHLILuPWKaWgrOolnjNnzkh/f78cOXKk4Prjx48lnU5LX1+fc83n80lvb69MTk6KiEgikZC3b98WrAkGg9Ld3e2sWW5paUmy2WzBAJVEduFWZBe1quyCMj4+LslkUiKRSNFj6XRaRETa29sLrre3tzuPpdNpaWhokPXr1392zXKRSEQsy3Kms7Oz3G0DKyK7cCuyi1pVVkGZm5uT4eFh+e2336SxsfGz6zweT8HHqlp0bbmV1pw/f15s23Zmbm6unG0DJSO7cCuyi1pTVkFJJBKSyWQkFApJfX291NfXSzwel4sXL0p9fb3T4Jc38kwm4zwWCAQkl8vJixcvPrtmOZ/PJy0tLQUDVEprayvZhSuRXdSysgrK4cOHJZVKydTUlDO7d++WU6dOydTUlGzbtk0CgYDEYjHnc3K5nMTjcenp6RERkVAoJF6vt2DN/Py8TE9PO2uAtXT37l2yC1ciu6hpJf369go+fheP6r9vd7MsSycmJjSVSunJkyc/+Xa3jo4OvXHjhiaTST106BBvd2OqNh+/VZPsMm4assu4db7Z24w/tryg5PN5HR0d1UAgoD6fT/fv36+pVKrgc16/fq3hcFhbW1u1qalJjx07prOzsyV/TZ4oTCXnwxOF7DJuG7LLuHVKKSgeVVVxmWw2K5ZlVXsbqBG2ba/Z6+tkF5VEduFWpWSXv8UDAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgnLIKSiQSkT179ojf75cNGzbIiRMn5NGjRwVrVFUuXLggwWBQmpqa5MCBA/LgwYOCNUtLSzI0NCRtbW3S3Nwsx48flydPnnz9aYCvQHbhVmQXNUnLcPToUR0bG9Pp6WmdmprS/v5+3bx5s758+dJZE41G1e/369WrVzWVSunAwIBu3LhRs9mss2ZwcFA3bdqksVhMk8mkHjx4UHft2qXv3r0raR+2bauIMExFxrZtssu4csgu49b5kN2VlFVQlstkMioiGo/HVVU1n89rIBDQaDTqrHnz5o1alqWXL19WVdWFhQX1er06Pj7urHn69KnW1dXp9evXS/q6PFGYSo5t22SXceWQXcatU0pB+arfQbFtW0REWltbRUTk8ePHkk6npa+vz1nj8/mkt7dXJicnRUQkkUjI27dvC9YEg0Hp7u521iy3tLQk2Wy2YIBKIrtwK7KLWrXqgqKqMjIyIvv27ZPu7m4REUmn0yIi0t7eXrC2vb3deSydTktDQ4OsX7/+s2uWi0QiYlmWM52dnavdNvBJZBduRXZRq1ZdUMLhsNy/f19+//33osc8Hk/Bx6padG25ldacP39ebNt2Zm5ubrXbBlZEduFWZBe1ZlUFZWhoSK5duya3bt2Sjo4O53ogEBARKWrkmUzGafeBQEByuZy8ePHis2uW8/l80tLSUjBAJZFduBXZRa0qq6CoqoTDYZmYmJCbN29KV1dXweNdXV0SCAQkFos513K5nMTjcenp6RERkVAoJF6vt2DN/Py8TE9PO2uAtUZ24VZkFzWrpF/f/j+nT59Wy7L09u3bOj8/78yrV6+cNdFoVC3L0omJCU2lUnry5MlPvt2to6NDb9y4oclkUg8dOsTb3Ziqzcdv1SS7jJuG7DJunYq/zfhzX2hsbMxZk8/ndXR0VAOBgPp8Pt2/f7+mUqmCf+f169caDoe1tbVVm5qa9NixYzo7O1vyPniiMJWcD08Ussu4bcgu49YppaB4VFXFZbLZrFiWVe1toEbYtr1mr6+TXVQS2YVblZJd/hYPAAAwDgUFAAAYh4ICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADGoaAAAADjUFAAAIBxKCgAAMA4FBQAAGAcCgoAADAOBQUAABiHggIAAIxDQQEAAMahoAAAAONQUAAAgHEoKAAAwDgUFAAAYBwKCgAAMA4FBQAAGIeCAgAAjENBAQAAxqGgAAAA41BQAACAcapaUC5duiRdXV3S2NgooVBI7ty5U83tACUju3ArsgvX0CoZHx9Xr9erV65c0YcPH+rw8LA2NzfrzMzMFz/Xtm0VEYapyNi2TXYZVw7ZZdw6pWS3agVl7969Ojg4WHBtx44deu7cuS9+Lk8UppJT7n/kyS5jypBdxq1TSnar8hJPLpeTRCIhfX19Bdf7+vpkcnKyaP3S0pJks1lnbNteq63iP0BVS15LdmESsgu3KiW7VSkoz58/l/fv30t7e3vB9fb2dkmn00XrI5GIWJblzObNm9dqq/gPWFxcLHkt2YVJyC7cqpTs1q/BPj7L4/EUfKyqRddERM6fPy8jIyPOxwsLC7JlyxaZnZ0Vy7K++T6rIZvNSmdnp8zNzUlLS0u1t/NNVPuMqiqLi4sSDAbL/tzVZjefz8vMzIx899133FuXq+YZye63Q3a/rXKyW5WC0tbWJuvWrStq7ZlMpqjdi4j4fD7x+XxF1y3LqtkAfdDS0sIZv6FyC24lsltX9+8PLrm3taFaZyS73xZn/HZKzW5VXuJpaGiQUCgksVis4HosFpOenp5qbAkoCdmFW5FduE3VXuIZGRmRH3/8UXbv3i0//PCD/PLLLzI7OyuDg4PV2hJQErILtyK7cJOqFZSBgQH5559/5KeffpL5+Xnp7u6WP/74Q7Zs2fLFz/X5fDI6OvrJl31qBWc019dkV8S95y4HZzQT2f0yzmgOj5bzPjUAAIA1wN/iAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHFcWlEuXLklXV5c0NjZKKBSSO3fuVHtLJYlEIrJnzx7x+/2yYcMGOXHihDx69KhgjarKhQsXJBgMSlNTkxw4cEAePHhQsGZpaUmGhoakra1Nmpub5fjx4/LkyZO1PErJIpGIeDweOXv2rHOt1s5YDrLrnvtKdguRXffc15rJbql/ptsU4+Pj6vV69cqVK/rw4UMdHh7W5uZmnZmZqfbWvujo0aM6Njam09PTOjU1pf39/bp582Z9+fKlsyYajarf79erV69qKpXSgYEB3bhxo2azWWfN4OCgbtq0SWOxmCaTST148KDu2rVL3717V41jfda9e/d069atunPnTh0eHnau19IZy0F23XNfyW4hsuue+1pL2XVdQdm7d68ODg4WXNuxY4eeO3euSjtavUwmoyKi8XhcVVXz+bwGAgGNRqPOmjdv3qhlWXr58mVVVV1YWFCv16vj4+POmqdPn2pdXZ1ev359bQ+wgsXFRd2+fbvGYjHt7e11nii1dMZykV133FeyW4zsuuO+1lp2XfUSTy6Xk0QiIX19fQXX+/r6ZHJyskq7Wj3btkVEpLW1VUREHj9+LOl0uuB8Pp9Pent7nfMlEgl5+/ZtwZpgMCjd3d1GfQ/OnDkj/f39cuTIkYLrtXTGcpBd99xXsluI7LrnvtZadqv2v7pfjefPn8v79++L/vJme3t70V/oNJ2qysjIiOzbt0+6u7tFRJwzfOp8MzMzzpqGhgZZv3590RpTvgfj4+OSTCblr7/+KnqsVs5YLrLrjvtKdouRXXfc11rMrqsKygcej6fgY1Utuma6cDgs9+/fl7t37xY9tprzmfI9mJubk+HhYfnzzz+lsbHxs+vcfMavQXaLmfI9ILsrI7vFTPke1Gp2XfUST1tbm6xbt66ozWUymaJmaLKhoSG5du2a3Lp1Szo6OpzrgUBARGTF8wUCAcnlcvLixYvPrqmmRCIhmUxGQqGQ1NfXS319vcTjcbl48aLU19c7e3TzGVeD7Jp/X8nup5Fd8+9rrWbXVQWloaFBQqGQxGKxguuxWEx6enqqtKvSqaqEw2GZmJiQmzdvSldXV8HjXV1dEggECs6Xy+UkHo875wuFQuL1egvWzM/Py/T0tBHfg8OHD0sqlZKpqSlndu/eLadOnZKpqSnZtm2b68+4GmTX/PtKdj+N7Jp/X2s2u2v6K7kV8OHtbr/++qs+fPhQz549q83Nzfr3339Xe2tfdPr0abUsS2/fvq3z8/POvHr1ylkTjUbVsiydmJjQVCqlJ0+e/ORbwTo6OvTGjRuaTCb10KFDRr7d7YOPf5tctTbPWAqy6777Snb/RXbdd19rIbuuKyiqqj///LNu2bJFGxoa9Pvvv3feLmY6EfnkjI2NOWvy+byOjo5qIBBQn8+n+/fv11QqVfDvvH79WsPhsLa2tmpTU5MeO3ZMZ2dn1/g0pVv+RKnFM5aK7LrrvpLd/0d23XVfayG7HlXVtf6pDQAAwEpc9TsoAADgv4GCAgAAjENBAQAAxqGgAAAA41BQAACAcSgoAADAOBQUAABgHAoKAAAwDgUFAAAYh4ICAACMQ0EBAADG+R/YeqzCPLmLOgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 顯示各軸切面\n",
    "b_img = get_bounding_box(data)\n",
    "print('image (tumor):', np.where(b_img == 2))\n",
    "\n",
    "show_slices([data[:, :, 187], data[:, :, 188], data[:, :, 189]])\n",
    "show_slices([b_img[:, :, 0], b_img[:, :, 1], b_img[:, :, 2]])\n",
    "# show_slices([resize_data[:, :, 29], resize_data[:, :, 30], resize_data[:, :, 48]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import label data\n",
    "with open('C:\\\\Users\\\\Gina\\\\Lab\\\\kidney\\\\kits21\\\\kits21\\\\data\\\\kits.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "label = [ case['tumor_histologic_subtype'] for case in data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useless img: [17, 18, 19, 20, 31, 42, 43, 56, 77, 82, 88, 90, 92, 104, 106, 116, 120, 133, 137, 155, 156, 176, 183, 188, 200, 202, 205, 211, 225, 228, 234, 237, 238, 249, 253, 258, 275, 277, 279, 288, 291]\n"
     ]
    }
   ],
   "source": [
    "useless_type = ['rcc_unclassified',\n",
    "                'urothelial',\n",
    "                'mest',\n",
    "                'collecting_duct_undefined',\n",
    "                'oncocytoma',\n",
    "                'clear_cell_papillary_rcc', \n",
    "                'multilocular_cystic_rcc', \n",
    "                'other', 'wilms', \n",
    "                'angiomyolipoma', 'spindle_cell_neoplasm']\n",
    "\n",
    "drop_idx_img = []\n",
    "for idx, value in enumerate(label):\n",
    "    if value in useless_type:\n",
    "        drop_idx_img.append(idx)\n",
    "\n",
    "print('Useless img:', drop_idx_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  input mask\n",
    "data_dir = 'C:\\\\Users\\\\Gina\\\\Lab\\\\kidney\\\\nnUNet-1\\\\Result'\n",
    "mask_path = []\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    if \"nii.gz\" in file:\n",
    "        if int(file[5:10]) not in drop_idx_img:\n",
    "            mask_path.append(os.path.join(data_dir, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gina\\AppData\\Local\\Temp\\ipykernel_21364\\3453872739.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mask = np.array([\n"
     ]
    }
   ],
   "source": [
    "mask = np.array([\n",
    "    get_tumor_region(path) for path in mask_path\n",
    "    ])\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.delete(label, drop_idx_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "label_dummy = pd.get_dummies(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chromophobe</th>\n",
       "      <th>clear_cell_rcc</th>\n",
       "      <th>papillary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>259 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     chromophobe  clear_cell_rcc  papillary\n",
       "0              0               1          0\n",
       "1              0               0          1\n",
       "2              0               0          1\n",
       "3              1               0          0\n",
       "4              0               1          0\n",
       "..           ...             ...        ...\n",
       "254            0               1          0\n",
       "255            0               1          0\n",
       "256            0               1          0\n",
       "257            0               1          0\n",
       "258            0               0          1\n",
       "\n",
       "[259 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train and validation are 207 and 52.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(mask, label_dummy, test_size = 0.2)\n",
    "\n",
    "print(\n",
    "    \"Number of samples in train and validation are %d and %d.\"\n",
    "    % (x_train.shape[0], x_val.shape[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chromophobe</th>\n",
       "      <th>clear_cell_rcc</th>\n",
       "      <th>papillary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     chromophobe  clear_cell_rcc  papillary\n",
       "4              0               1          0\n",
       "32             0               1          0\n",
       "196            0               1          0\n",
       "14             0               1          0\n",
       "49             0               1          0\n",
       "..           ...             ...        ...\n",
       "118            0               1          0\n",
       "176            0               1          0\n",
       "209            0               1          0\n",
       "174            0               1          0\n",
       "193            0               1          0\n",
       "\n",
       "[207 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def rotate(volume):\n",
    "    \"\"\"Rotate the volume by a few degrees\"\"\"\n",
    "\n",
    "    def scipy_rotate(volume):\n",
    "        # define some rotation angles\n",
    "        angles = [-20, -10, -5, 5, 10, 20]\n",
    "        # pick angles at random\n",
    "        angle = random.choice(angles)\n",
    "        # rotate volume\n",
    "        volume = ndimage.rotate(volume, angle, reshape=False)\n",
    "        volume[volume < 0] = 0.0\n",
    "        volume[volume > 1] = 1.0\n",
    "        return volume\n",
    "\n",
    "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
    "    return augmented_volume\n",
    "\n",
    "\n",
    "def train_preprocessing(volume, label):\n",
    "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
    "    # Rotate volume\n",
    "    # volume = rotate(volume)\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    # print(volume.get_shape())\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return volume, label\n",
    "\n",
    "\n",
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return volume, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders.\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train.to_numpy()))\n",
    "validation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val.to_numpy()))\n",
    "\n",
    "batch_size = 2\n",
    "# Augment the on the fly during training.\n",
    "train_dataset = (\n",
    "    train_loader.shuffle(len(x_train))\n",
    "    .map(train_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")\n",
    "# Only rescale.\n",
    "validation_dataset = (\n",
    "    validation_loader.shuffle(len(x_val))\n",
    "    .map(validation_preprocessing)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"3dcnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 128, 128, 64, 1)] 0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 126, 126, 62, 64)  1792      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 63, 63, 31, 64)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 63, 63, 31, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 61, 61, 29, 64)    110656    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 30, 30, 14, 64)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30, 30, 14, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 28, 28, 12, 128)   221312    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 14, 14, 6, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 14, 14, 6, 128)    512       \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 12, 12, 4, 256)    884992    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 6, 6, 2, 256)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 2, 256)      1024      \n",
      "_________________________________________________________________\n",
      "global_average_pooling3d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 1,353,923\n",
      "Trainable params: 1,352,899\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(width=128, height=128, depth=64):\n",
    "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(units=3, activation=\"softmax\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model.\n",
    "model = get_model(width=128, height=128, depth=64)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "104/104 - 12s - loss: 0.5103 - acc: 0.7633 - val_loss: 0.3712 - val_acc: 0.8269\n",
      "Epoch 2/100\n",
      "104/104 - 7s - loss: 0.4752 - acc: 0.7778 - val_loss: 0.3819 - val_acc: 0.8269\n",
      "Epoch 3/100\n",
      "104/104 - 7s - loss: 0.4926 - acc: 0.7778 - val_loss: 0.3741 - val_acc: 0.8269\n",
      "Epoch 4/100\n",
      "104/104 - 6s - loss: 0.4408 - acc: 0.7778 - val_loss: 0.3908 - val_acc: 0.8269\n",
      "Epoch 5/100\n",
      "104/104 - 7s - loss: 0.4407 - acc: 0.7778 - val_loss: 0.3903 - val_acc: 0.8269\n",
      "Epoch 6/100\n",
      "104/104 - 6s - loss: 0.4226 - acc: 0.7778 - val_loss: 0.3997 - val_acc: 0.8269\n",
      "Epoch 7/100\n",
      "104/104 - 6s - loss: 0.4400 - acc: 0.7778 - val_loss: 0.3924 - val_acc: 0.8269\n",
      "Epoch 8/100\n",
      "104/104 - 6s - loss: 0.4283 - acc: 0.7778 - val_loss: 0.3774 - val_acc: 0.8269\n",
      "Epoch 9/100\n",
      "104/104 - 7s - loss: 0.4325 - acc: 0.7778 - val_loss: 0.3820 - val_acc: 0.8269\n",
      "Epoch 10/100\n",
      "104/104 - 7s - loss: 0.4280 - acc: 0.7778 - val_loss: 0.3838 - val_acc: 0.8269\n",
      "Epoch 11/100\n",
      "104/104 - 7s - loss: 0.4115 - acc: 0.7778 - val_loss: 0.3958 - val_acc: 0.8269\n",
      "Epoch 12/100\n",
      "104/104 - 7s - loss: 0.3968 - acc: 0.7778 - val_loss: 0.3876 - val_acc: 0.8269\n",
      "Epoch 13/100\n",
      "104/104 - 7s - loss: 0.4198 - acc: 0.7778 - val_loss: 0.3900 - val_acc: 0.8269\n",
      "Epoch 14/100\n",
      "104/104 - 7s - loss: 0.4167 - acc: 0.7778 - val_loss: 0.3738 - val_acc: 0.8269\n",
      "Epoch 15/100\n",
      "104/104 - 7s - loss: 0.4063 - acc: 0.7778 - val_loss: 0.3688 - val_acc: 0.8269\n",
      "Epoch 16/100\n",
      "104/104 - 6s - loss: 0.3850 - acc: 0.7778 - val_loss: 0.3658 - val_acc: 0.8269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4cb7da880>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model.\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "# Define callbacks.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"3d_image_classification.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "epochs = 100\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 . train: [0.7632850408554077, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544]\n",
      "0 . val: [0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042, 0.8269230723381042]\n",
      "1 . train: [0.5102962851524353, 0.4751507341861725, 0.4926392138004303, 0.44076257944107056, 0.44069966673851013, 0.42261409759521484, 0.43998831510543823, 0.42833635210990906, 0.4325442910194397, 0.4280364215373993, 0.41146647930145264, 0.39679163694381714, 0.41983562707901, 0.41670456528663635, 0.4063231647014618, 0.3850000500679016]\n",
      "1 . val: [0.37115147709846497, 0.38193413615226746, 0.37413379549980164, 0.3908488154411316, 0.3902808427810669, 0.39971810579299927, 0.3923972249031067, 0.3773631751537323, 0.3820313811302185, 0.38382911682128906, 0.3958274722099304, 0.3876244127750397, 0.39003169536590576, 0.37378689646720886, 0.3688470125198364, 0.36580413579940796]\n"
     ]
    }
   ],
   "source": [
    "# fig, ax = plt.subplots(1, 2, figsize=(20, 3))\n",
    "# ax = ax.ravel()\n",
    "\n",
    "for i, metric in enumerate([\"acc\", \"loss\"]):\n",
    "    print(i, '. train:', model.history.history[metric])\n",
    "    print(i, '. val:', model.history.history[\"val_\" + metric])\n",
    "    # ax[i].plot(model.history.history[metric])\n",
    "    # ax[i].plot(model.history.history[\"val_\" + metric])\n",
    "    # ax[i].set_title(\"Model {}\".format(metric))\n",
    "    # ax[i].set_xlabel(\"epochs\")\n",
    "    # ax[i].set_ylabel(metric)\n",
    "    # ax[i].legend([\"train\", \"val\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(train_dataset), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"3dcnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128, 128, 64, 1)] 0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 126, 126, 62, 64)  1792      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 63, 63, 31, 64)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 63, 63, 31, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 61, 61, 29, 128)   221312    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 30, 30, 14, 128)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 30, 30, 14, 128)   512       \n",
      "_________________________________________________________________\n",
      "global_average_pooling3d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 257,667\n",
      "Trainable params: 257,283\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model_2(width=128, height=128, depth=64):\n",
    "    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "    inputs = keras.Input((width, height, depth, 1))\n",
    "\n",
    "    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(units=256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(units=3, activation=\"softmax\")(x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model.\n",
    "model2 = get_model_2(width=128, height=128, depth=64)\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "104/104 - 8s - loss: 1.0117 - acc: 0.7391 - auc: 0.8171 - val_loss: 0.9484 - val_acc: 0.8269 - val_auc: 0.8889\n",
      "Epoch 2/100\n",
      "104/104 - 7s - loss: 0.9038 - acc: 0.7778 - auc: 0.8262 - val_loss: 0.8526 - val_acc: 0.8269 - val_auc: 0.8614\n",
      "Epoch 3/100\n",
      "104/104 - 7s - loss: 0.8582 - acc: 0.7778 - auc: 0.8262 - val_loss: 0.6793 - val_acc: 0.8269 - val_auc: 0.8475\n",
      "Epoch 4/100\n",
      "104/104 - 7s - loss: 0.7884 - acc: 0.7778 - auc: 0.8355 - val_loss: 0.6102 - val_acc: 0.8269 - val_auc: 0.8413\n",
      "Epoch 5/100\n",
      "104/104 - 7s - loss: 0.7810 - acc: 0.7778 - auc: 0.8262 - val_loss: 0.6398 - val_acc: 0.8269 - val_auc: 0.8466\n",
      "Epoch 6/100\n",
      "104/104 - 7s - loss: 0.7367 - acc: 0.7778 - auc: 0.8428 - val_loss: 0.6476 - val_acc: 0.8269 - val_auc: 0.8476\n",
      "Epoch 7/100\n",
      "104/104 - 7s - loss: 0.7394 - acc: 0.7778 - auc: 0.8270 - val_loss: 0.6708 - val_acc: 0.8269 - val_auc: 0.8744\n",
      "Epoch 8/100\n",
      "104/104 - 7s - loss: 0.7184 - acc: 0.7778 - auc: 0.8290 - val_loss: 0.5921 - val_acc: 0.8269 - val_auc: 0.8944\n",
      "Epoch 9/100\n",
      "104/104 - 7s - loss: 0.6994 - acc: 0.7778 - auc: 0.8343 - val_loss: 0.6111 - val_acc: 0.8269 - val_auc: 0.8710\n",
      "Epoch 10/100\n",
      "104/104 - 7s - loss: 0.6940 - acc: 0.7778 - auc: 0.8416 - val_loss: 0.9259 - val_acc: 0.8269 - val_auc: 0.8807\n",
      "Epoch 11/100\n",
      "104/104 - 7s - loss: 0.6977 - acc: 0.7778 - auc: 0.8432 - val_loss: 0.7118 - val_acc: 0.8269 - val_auc: 0.8466\n",
      "Epoch 12/100\n",
      "104/104 - 7s - loss: 0.6938 - acc: 0.7778 - auc: 0.8260 - val_loss: 0.6092 - val_acc: 0.8269 - val_auc: 0.8878\n",
      "Epoch 13/100\n",
      "104/104 - 7s - loss: 0.6947 - acc: 0.7778 - auc: 0.8280 - val_loss: 0.7823 - val_acc: 0.8269 - val_auc: 0.8883\n",
      "Epoch 14/100\n",
      "104/104 - 7s - loss: 0.6787 - acc: 0.7778 - auc: 0.8557 - val_loss: 0.6771 - val_acc: 0.8269 - val_auc: 0.8975\n",
      "Epoch 15/100\n",
      "104/104 - 7s - loss: 0.6782 - acc: 0.7778 - auc: 0.8498 - val_loss: 1.1765 - val_acc: 0.8269 - val_auc: 0.8702\n",
      "Epoch 16/100\n",
      "104/104 - 7s - loss: 0.6704 - acc: 0.7778 - auc: 0.8634 - val_loss: 0.6246 - val_acc: 0.8269 - val_auc: 0.8980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4ef614ac0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model.\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "model2.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=[\"acc\", tf.keras.metrics.AUC()],\n",
    ")\n",
    "\n",
    "# Define callbacks.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"3d_image_classification_2.h5\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "epochs = 100\n",
    "model2.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09/08\n",
    "* 可以改成分 Low grade, High grade，應該比較好分\n",
    "* 先把tumor這塊切出來，Cube 包起 tumor，只用這 cube，找最小的bounding box，再拉出來看"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea397cec91cadfeba1f637b1e229fc5613e21e348c4b388b4145c8bc2692c566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
